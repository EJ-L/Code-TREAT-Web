{
  "task": "code translation",
  "filterMappings": {
    "overall": {
      "modality": [],
      "knowledge": [],
      "reasoning": [],
      "dataset": []
    },
    "reasoning-Direct": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct"
      ],
      "dataset": []
    },
    "reasoning-Direct-CoTReasoning": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct",
        "CoT Reasoning"
      ],
      "dataset": []
    },
    "dataset-HackerRank": {
      "modality": [],
      "knowledge": [],
      "reasoning": [],
      "dataset": [
        "HackerRank"
      ]
    },
    "dataset-PolyHumanEval": {
      "modality": [],
      "knowledge": [],
      "reasoning": [],
      "dataset": [
        "PolyHumanEval"
      ]
    },
    "dataset-HackerRank-PolyHumanEval": {
      "modality": [],
      "knowledge": [],
      "reasoning": [],
      "dataset": [
        "HackerRank",
        "PolyHumanEval"
      ]
    },
    "dataset-HackerRank_reasoning-Direct": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct"
      ],
      "dataset": [
        "HackerRank"
      ]
    },
    "dataset-PolyHumanEval_reasoning-Direct": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct"
      ],
      "dataset": [
        "PolyHumanEval"
      ]
    },
    "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct"
      ],
      "dataset": [
        "HackerRank",
        "PolyHumanEval"
      ]
    },
    "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct",
        "CoT Reasoning"
      ],
      "dataset": [
        "HackerRank"
      ]
    },
    "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct",
        "CoT Reasoning"
      ],
      "dataset": [
        "PolyHumanEval"
      ]
    },
    "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
      "modality": [],
      "knowledge": [],
      "reasoning": [
        "Direct",
        "CoT Reasoning"
      ],
      "dataset": [
        "HackerRank",
        "PolyHumanEval"
      ]
    }
  },
  "data": {
    "DeepSeek-R1": {
      "overall": {
        "rank": 1,
        "pass@1": "79.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 1,
        "pass@1": "79.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 1,
        "pass@1": "79.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 1,
        "pass@1": "79.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 8,
        "pass@1": "80.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 1,
        "pass@1": "79.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 1,
        "pass@1": "79.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 8,
        "pass@1": "80.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 1,
        "pass@1": "79.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 1,
        "pass@1": "79.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 8,
        "pass@1": "80.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 1,
        "pass@1": "79.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "o3-mini (Med)": {
      "overall": {
        "rank": 2,
        "pass@1": "76.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 2,
        "pass@1": "76.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 2,
        "pass@1": "76.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 6,
        "pass@1": "72.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 4,
        "pass@1": "85.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 2,
        "pass@1": "76.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 6,
        "pass@1": "72.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 4,
        "pass@1": "85.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 2,
        "pass@1": "76.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 6,
        "pass@1": "72.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 4,
        "pass@1": "85.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 2,
        "pass@1": "76.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "o4-mini (Med)": {
      "overall": {
        "rank": 3,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 3,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 3,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 8,
        "pass@1": "68.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 1,
        "pass@1": "90.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 3,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 8,
        "pass@1": "68.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 1,
        "pass@1": "90.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 3,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 8,
        "pass@1": "68.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 1,
        "pass@1": "90.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 3,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "DeepSeek-R1 (0528)": {
      "overall": {
        "rank": 4,
        "pass@1": "72.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 4,
        "pass@1": "72.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 4,
        "pass@1": "72.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 3,
        "pass@1": "76.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 17,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 4,
        "pass@1": "72.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 3,
        "pass@1": "76.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 17,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 4,
        "pass@1": "72.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 3,
        "pass@1": "76.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 17,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 4,
        "pass@1": "72.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "GPT-4o-2024-11-20": {
      "overall": {
        "rank": 5,
        "pass@1": "71.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 5,
        "pass@1": "71.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 5,
        "pass@1": "71.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 12,
        "pass@1": "64.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 3,
        "pass@1": "85.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 5,
        "pass@1": "71.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 12,
        "pass@1": "64.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 3,
        "pass@1": "85.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 5,
        "pass@1": "71.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 12,
        "pass@1": "64.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 3,
        "pass@1": "85.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 5,
        "pass@1": "71.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Gemini-2.5-Pro-Preview-05-06": {
      "overall": {
        "rank": 6,
        "pass@1": "70.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 6,
        "pass@1": "70.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 6,
        "pass@1": "70.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 4,
        "pass@1": "75.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 21,
        "pass@1": "60.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 6,
        "pass@1": "70.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 4,
        "pass@1": "75.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 21,
        "pass@1": "60.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 6,
        "pass@1": "70.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 4,
        "pass@1": "75.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 21,
        "pass@1": "60.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 6,
        "pass@1": "70.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Claude-Sonnet-4": {
      "overall": {
        "rank": 7,
        "pass@1": "70.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 7,
        "pass@1": "70.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 7,
        "pass@1": "70.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 2,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 23,
        "pass@1": "58.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 7,
        "pass@1": "70.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 2,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 23,
        "pass@1": "58.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 7,
        "pass@1": "70.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 2,
        "pass@1": "76.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 23,
        "pass@1": "58.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 7,
        "pass@1": "70.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "DeepSeek-V3": {
      "overall": {
        "rank": 8,
        "pass@1": "70.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 8,
        "pass@1": "70.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 8,
        "pass@1": "70.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 13,
        "pass@1": "63.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 7,
        "pass@1": "83.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 8,
        "pass@1": "70.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 13,
        "pass@1": "63.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 7,
        "pass@1": "83.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 8,
        "pass@1": "70.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 13,
        "pass@1": "63.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 7,
        "pass@1": "83.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 8,
        "pass@1": "70.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Claude-3.7-Sonnet": {
      "overall": {
        "rank": 9,
        "pass@1": "69.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 9,
        "pass@1": "69.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 9,
        "pass@1": "69.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 10,
        "pass@1": "67.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 13,
        "pass@1": "72.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 9,
        "pass@1": "69.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 10,
        "pass@1": "67.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 13,
        "pass@1": "72.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 9,
        "pass@1": "69.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 10,
        "pass@1": "67.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 13,
        "pass@1": "72.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 9,
        "pass@1": "69.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Qwen3-235B-A22B": {
      "overall": {
        "rank": 10,
        "pass@1": "67.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 10,
        "pass@1": "67.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 10,
        "pass@1": "67.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 7,
        "pass@1": "71.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 22,
        "pass@1": "60.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 10,
        "pass@1": "67.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 7,
        "pass@1": "71.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 22,
        "pass@1": "60.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 10,
        "pass@1": "67.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 7,
        "pass@1": "71.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 22,
        "pass@1": "60.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 10,
        "pass@1": "67.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "GPT-4-turbo-2024-04-09": {
      "overall": {
        "rank": 11,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 11,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 11,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 15,
        "pass@1": "58.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 6,
        "pass@1": "83.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 11,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 15,
        "pass@1": "58.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 6,
        "pass@1": "83.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 11,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 15,
        "pass@1": "58.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 6,
        "pass@1": "83.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 11,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "GPT-4.1-2025-04-14": {
      "overall": {
        "rank": 12,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 12,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 12,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 5,
        "pass@1": "72.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 24,
        "pass@1": "57.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 12,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 5,
        "pass@1": "72.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 24,
        "pass@1": "57.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 12,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 5,
        "pass@1": "72.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 24,
        "pass@1": "57.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 12,
        "pass@1": "67.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Grok-3-Mini (High)": {
      "overall": {
        "rank": 13,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 13,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 13,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 11,
        "pass@1": "66.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 18,
        "pass@1": "64.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 13,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 11,
        "pass@1": "66.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 18,
        "pass@1": "64.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 13,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 11,
        "pass@1": "66.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 18,
        "pass@1": "64.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 13,
        "pass@1": "66.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Qwen3-32B": {
      "overall": {
        "rank": 14,
        "pass@1": "65.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 14,
        "pass@1": "65.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 14,
        "pass@1": "65.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 9,
        "pass@1": "68.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 20,
        "pass@1": "60.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 14,
        "pass@1": "65.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 9,
        "pass@1": "68.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 20,
        "pass@1": "60.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 14,
        "pass@1": "65.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 9,
        "pass@1": "68.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 20,
        "pass@1": "60.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 14,
        "pass@1": "65.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Qwen2.5-Coder-32B-Instruct": {
      "overall": {
        "rank": 15,
        "pass@1": "64.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 15,
        "pass@1": "64.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 15,
        "pass@1": "64.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 16,
        "pass@1": "53.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 2,
        "pass@1": "86.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 15,
        "pass@1": "64.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 16,
        "pass@1": "53.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 2,
        "pass@1": "86.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 15,
        "pass@1": "64.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 16,
        "pass@1": "53.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 2,
        "pass@1": "86.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 15,
        "pass@1": "64.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Qwen3-30B-A3B": {
      "overall": {
        "rank": 16,
        "pass@1": "63.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 16,
        "pass@1": "63.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 16,
        "pass@1": "63.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 14,
        "pass@1": "61.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 15,
        "pass@1": "67.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 16,
        "pass@1": "63.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 14,
        "pass@1": "61.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 15,
        "pass@1": "67.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 16,
        "pass@1": "63.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 14,
        "pass@1": "61.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 15,
        "pass@1": "67.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 16,
        "pass@1": "63.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Qwen2.5-72B-Instruct": {
      "overall": {
        "rank": 17,
        "pass@1": "62.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 17,
        "pass@1": "62.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 17,
        "pass@1": "62.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 18,
        "pass@1": "50.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 5,
        "pass@1": "84.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 17,
        "pass@1": "62.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 18,
        "pass@1": "50.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 5,
        "pass@1": "84.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 17,
        "pass@1": "62.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 18,
        "pass@1": "50.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 5,
        "pass@1": "84.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 17,
        "pass@1": "62.3",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Llama-3.3-70B-Instruct": {
      "overall": {
        "rank": 18,
        "pass@1": "60.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 18,
        "pass@1": "60.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 18,
        "pass@1": "60.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 17,
        "pass@1": "51.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 11,
        "pass@1": "78.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 18,
        "pass@1": "60.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 17,
        "pass@1": "51.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 11,
        "pass@1": "78.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 18,
        "pass@1": "60.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 17,
        "pass@1": "51.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 11,
        "pass@1": "78.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 18,
        "pass@1": "60.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Llama-3.1-70B-Instruct": {
      "overall": {
        "rank": 19,
        "pass@1": "56.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 19,
        "pass@1": "56.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 19,
        "pass@1": "56.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 19,
        "pass@1": "44.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 9,
        "pass@1": "80.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 19,
        "pass@1": "56.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 19,
        "pass@1": "44.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 9,
        "pass@1": "80.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 19,
        "pass@1": "56.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 19,
        "pass@1": "44.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 9,
        "pass@1": "80.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 19,
        "pass@1": "56.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Claude-3.5-Haiku-20241022": {
      "overall": {
        "rank": 20,
        "pass@1": "55.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 20,
        "pass@1": "55.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 20,
        "pass@1": "55.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 21,
        "pass@1": "43.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 10,
        "pass@1": "80.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 20,
        "pass@1": "55.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 21,
        "pass@1": "43.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 10,
        "pass@1": "80.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 20,
        "pass@1": "55.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 21,
        "pass@1": "43.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 10,
        "pass@1": "80.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 20,
        "pass@1": "55.7",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Llama-4-Scout-17B-16E-Instruct": {
      "overall": {
        "rank": 21,
        "pass@1": "53.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 21,
        "pass@1": "53.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 21,
        "pass@1": "53.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 23,
        "pass@1": "42.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 12,
        "pass@1": "74.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 21,
        "pass@1": "53.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 23,
        "pass@1": "42.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 12,
        "pass@1": "74.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 21,
        "pass@1": "53.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 23,
        "pass@1": "42.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 12,
        "pass@1": "74.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 21,
        "pass@1": "53.6",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Claude-3.5-Sonnet-20241022": {
      "overall": {
        "rank": 22,
        "pass@1": "52.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 22,
        "pass@1": "52.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 22,
        "pass@1": "52.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 22,
        "pass@1": "42.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 14,
        "pass@1": "70.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 22,
        "pass@1": "52.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 22,
        "pass@1": "42.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 14,
        "pass@1": "70.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 22,
        "pass@1": "52.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 22,
        "pass@1": "42.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 14,
        "pass@1": "70.0",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 22,
        "pass@1": "52.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "GPT-3.5-turbo-0125": {
      "overall": {
        "rank": 23,
        "pass@1": "51.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 23,
        "pass@1": "51.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 23,
        "pass@1": "51.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 20,
        "pass@1": "44.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 19,
        "pass@1": "64.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 23,
        "pass@1": "51.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 20,
        "pass@1": "44.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 19,
        "pass@1": "64.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 23,
        "pass@1": "51.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 20,
        "pass@1": "44.4",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 19,
        "pass@1": "64.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 23,
        "pass@1": "51.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Llama-3.1-8B-Instruct": {
      "overall": {
        "rank": 24,
        "pass@1": "40.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 24,
        "pass@1": "40.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 24,
        "pass@1": "40.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 25,
        "pass@1": "26.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 16,
        "pass@1": "67.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 24,
        "pass@1": "40.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 25,
        "pass@1": "26.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 16,
        "pass@1": "67.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 24,
        "pass@1": "40.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 25,
        "pass@1": "26.1",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 16,
        "pass@1": "67.9",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 24,
        "pass@1": "40.5",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    },
    "Gemma-3-27B-Instruct": {
      "overall": {
        "rank": 25,
        "pass@1": "36.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct": {
        "rank": 25,
        "pass@1": "36.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "reasoning-Direct-CoTReasoning": {
        "rank": 25,
        "pass@1": "36.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank": {
        "rank": 24,
        "pass@1": "41.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval": {
        "rank": 25,
        "pass@1": "27.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval": {
        "rank": 25,
        "pass@1": "36.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct": {
        "rank": 24,
        "pass@1": "41.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct": {
        "rank": 25,
        "pass@1": "27.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct": {
        "rank": 25,
        "pass@1": "36.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank_reasoning-Direct-CoTReasoning": {
        "rank": 24,
        "pass@1": "41.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 25,
        "pass@1": "27.2",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      },
      "dataset-HackerRank-PolyHumanEval_reasoning-Direct-CoTReasoning": {
        "rank": 25,
        "pass@1": "36.8",
        "pass@3": "-",
        "pass@5": "-",
        "CodeBLEU": "-"
      }
    }
  }
}